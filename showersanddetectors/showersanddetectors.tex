% !TEX root = ../main.tex
\begin{singlespace}
\chapter{Extensive Air Showers and Cosmic Ray Detectors} %particle physics basics, weighted moments, elongation rate, transport equations, Xmax, LDF, parton theory, GZK, heitler model, obviously not in that order}
\end{singlespace}
Through this chapter, I plan to give a description of some older and some more modern theories of how to quantify the important aspects of the air showers caused by Ultra High Energy Cosmic Rays. This will lead into a discussion of the various methods that have been employed to detect extensive air showers, and an overview of a few important experiments that employ them.
\section{Extensive Air Showers}
When an atomic nuclei or single proton of $10^{18}+$ eV (frequently called the primary) comes crashing into the upper atmosphere, particle physics at the very edge of humanity's current understanding ensues. After a violent disk of plasma created by slightly-beyond-the-verified-standard-model processes cools out into high energy Large Hadron Collider level interactions, the shower is carried down in phenomena quantified by a clever mix of particle physics and statistical mechanics. The first $10^{-15}$-or-so seconds of this process (after the extrapolated standard model physics) are described by the basic principles of particles physics, and so the treatment in this chapter will start there. After this point, we typically become more concerned with how the particles ``transport'' from one species to another, and how much energy is deposited in the nearby air. The rate at which particles are moved from pure kinetic energy of the prompt secondaries into matter in the later secondaries is a main parameter of cosmic ray showers and its maximum, $X_{max}$, is perhaps the best known predictor of the primary particle's composition.

After the shower undergoes its maximum of particle production, we usually become more concerned with the observable parameters on the ground, which can be seen by typical particle counting or calorimetric detectors. These observables are, in the case of the surface detector stations of Auger, derived from the signal recorded by one or more types of ground based detection. From this, a number of important observables can be extrapolated, however two of central importance are the muon-to-electron ratio ($N_{\mu}/N_e$) and the lateral density function (LDF). $N_{\mu}/N_e$ can be derived from a single analog to digital converter (ADC) trace, or more accurately derived from multiple ADC traces from two or more types of detectors, ideally with different responses to muons and electrons, such as a scintillator and a Water Cherenkov Detector (WCD).

In the subsequent sections of this chapter, we summarize this process at each level. On the topic of particles physics, we present a treatment which discusses the steps for calculating decay widths and cross sections of the electroweak force, but does not delve into the group theoretic fundamental structure underlying the modern theories of particles physics. It should be noted, the mathematics used to describe these extensive air showers (or just ``showers'') was derived for lower energy primaries. After the initial higher energy processes, much of it is still entirely valid, but to tie the chapter into how this work is actually done, we include a discussion of the premier air shower simulation package, CORSIKA, which is almost always used instead of hand calculations for modern experimental design, verification and data analysis.
\subsection{Particle Physics Basics}
To start from the beginning of the story, particle physics as we know it today, stems from the 1928 equation by Dirac \cite{dirac}:
\begeq{i \slashed{\partial}\psi=m\psi. \label{diraceq}}
Outside of $i$, the square root of negative one, and $m$, the mass of the particle, each of these objects will need an explanation. The first, $\slashed{\partial}=\gamma^\mu \partial_\mu=\gamma^\nu g_{\mu\nu} \partial^\mu$, is the four dimensional partial derivative operator contracted with a set of the ``gamma matrices'' ($\gamma^\mu$). There are a number of bases we can put these in, but the two most common are the Dirac basis and the Weyl basis. The Dirac basis is used  for most calculations, while the Weyl basis is typically used when discussing the chirality of particles, in which case it can be used to decouple the Dirac equation into two easily solved second order differential equations. 

Going back to \autoref{diraceq}, $\psi$ is an object called a bi-spinor, and represents the particle's wavefunction. A spinor, is a vector with two complex entries which transforms under the SU(2) group. Frequently, these spinors are required to have magnitude 1, and so a bi-spinor is a four complex entry vector that mechanically looks something like:
$$\begin{pmatrix}\begin{pmatrix}
a \\
b
\end{pmatrix}\\
\begin{pmatrix}
c \\
d
\end{pmatrix}\end{pmatrix},$$
a mathematical structure which is required by the Dirac equation. In general, we impose this normalization condition on the wavefunction $\psi$:
$$\bar{\psi}\psi=\psi^\dagger\gamma^0\psi=1.$$
Above, the $\dagger$ represents the hermitian conjugate, and $\gamma^0$ is the zeroth member of Dirac's gamma matrices. In connection with Schr{\"o}dinger's quantum mechanics, the entries of these bi-spinors are frequently simple plane waves and represent both the spatial and temporal behavior of the particles.

From a utilitarian perspective, these bi-spinors and the results of the Dirac equation become useful when seen through the standard model Lagrangians, Feynman diagrams, and ultimately the path integral formulation \cite{crapp}. While the path integral formulation is certainly the most powerful tool for calculating cross sections and decay widths, a discussion of how to use it is outside the experimentally-focused nature of this work. Let's proceed by discussing some of the basic processes that occur in the detection of astrophysical particles from a basic particle physics view.

The Lagrangian is, at the lowest level of complexity, a statement of a classical systems total energy. When we apply this concept to particles of the standard model however, we see that it becomes a statement of the basic possible interactions available through each force \cite{cottingham}. For example, when we look at the Lagrangian density statement for the weak force as it pertains to interactions between electrons and neutrinos, we find a term for the charged current interactions of electron neutrinos and electrons, and its quantized operator structure like this:
\begin{figure}
\begin{center}
\begin{tabular}{lccc}

Term: &$\mathscr{L}_{EW}\supset-2\sqrt{2}G_{F}e^{\dag}_{L}\tilde{\sigma}^{\mu}\nu_{eL}e_{L}^{\dag}\tilde{\sigma}_{\mu}^{\dag}\nu_{eL}$ & \\
Operator Structure: & $e^{\dag}_{L}\nu_{eL}*\nu_{eL}^{\dag}e_{L}$ & \\ \\
&
\begin{fmffile}{evec1}
\begin{fmfgraph*}(50,50) 
\fmfstraight
\fmfleft{i1,i2}\fmfright{o1,o2}
\fmflabel{$\nu_{e}$}{i2}
\fmf{fermion}{i1,v1}
\fmf{fermion}{v1,o1}
\fmflabel{$\nu_{e}$}{o1}
\fmf{fermion}{i2,v2}
\fmflabel{$e^{-}$}{i1}
\fmf{fermion}{v2,o2}
\fmflabel{$e^{-}$}{o2}
\fmf{zigzag,label=$W^{-}$}{v1,v2}
\end{fmfgraph*}
\end{fmffile}
&

&
\begin{fmffile}{evec2}
\begin{fmfgraph*}(50,50) 
\fmfstraight
\fmfleft{i1,i2}\fmfright{o1,o2}
\fmflabel{$\nu_{e}$}{i1}
\fmf{fermion}{i1,v1}
\fmf{fermion}{v1,o1}
\fmflabel{$\nu_{e}$}{o2}
\fmf{fermion}{i2,v2}
\fmflabel{$e^{-}$}{i2}
\fmf{fermion}{v2,o2}
\fmflabel{$e^{-}$}{o1}
\fmf{zigzag,label=$W^{-}$}{v1,v2}
\end{fmfgraph*}
\end{fmffile}
\\


\end{tabular}
\end{center}
\caption[Feynman Diagrams and the Standard Model]{Here are two Feynman Diagrams and their corresponding terms in the electroweak Lagrangian density, along with the term's operator structure.}
\label{cc}
\end{figure}
In \autoref{cc}, the $e$ and $\nu$ terms are bi-spinors, as discussed earlier. The electroweak Lagrangian density, $\mathscr{L}_{EW}$, contains permutations of such interaction terms for each possible tree level (two vertex) interaction that can occur via the electroweak force. The interactions shown above involve neutrinos scattering off of electrons. These Feynman diagrams can be turned into a cross section calculation by tracing each vertex through to the end and multiplying each route together, and adding up the cross sections found by all possible topologically distinct permutations of the incoming and outgoing particles. Each vertex has a different term which we use to represent it, and this allows us to complete the calculation.

Once the expression for the diagram is assembled, we use the fact that the expression must produce a scalar quantity, and then invoke the Casimir trace theorem, which exposes the final result's sole dependence on the trace from the long expression of bi-spinors and gamma matrices. A number of these tricks are very well explained in \textcite{elempart}, which stands as a surprisingly good resource for many levels of students and researchers. 

While walking through a calculation may be just outside the scope of this introduction, we will leave the steps summarized below:
\begin{enumerate}
\item Draw diagram
\item Write down scattering amplitude, $\mathcal{M}$ by tracing through each outgoing particles path backwards, including the appropriate vertices and propagators as you go
\item Multiply $\mathcal{M}$ by its Hermitian Conjugate to obtain $|\mathcal{M}|^2$
\item Insert $\gamma^0\gamma^0$ next to adjoint ($\dagger$) operators to `undagger' them
\item Commute $\gamma$ matrices until you find a scalar term, i.e. $u(p_i,s)...\bar{u}(p_j,s)$
\item Permute the bi-spinors in the scalars ($u(p_i,s)$, etc.) to be next to each other, so you can sum over the spins to make the spin-summed square magnitude $\langle |\mathcal{M}|^2\rangle$
\item Take the traces and contract their results to produce dot and cross products of momenta (this can be done by FeynCalc\footnote{Additionally, I will note, as in the instructions above, I advocate using the FeynCalc (\cite{feyncalc1,feyncalc2}) package in Mathematica to complete these calculations if they are not for pedagogical purposes. It is certainly a rite of passage to complete a decay width or scattering cross section calculation by hand at some point, but for research purposes, modern symbolic computation system are advanced enough to save a researcher a large amount of time. The experimental values of many cross sections can be easily looked up in the Particle Data Group webpage (\url{http://pdg.lbl.gov}, \textcite{pdg}). } \cite{feyncalc1,feyncalc2})
\item Express the inner propagators in terms of outgoing momenta
\item Calculate kinematics, which directly translates to the arguments of the delta functions in $d\mbox{LIPS}_n$
\item Insert $\langle |\mathcal{M}|^2\rangle d\mbox{LIPS}_n$ into the scattering amplitude
\item Calculate the integral numerically (this can be done with Mathematica).
\end{enumerate}

\subsubsection{Moving Particle Physics into Shower Physics}
As we move from purely particle physics concepts into those that are applied to extensive air showers, we need to adopt a new formalism. Cosmics ray physicists studying showers, generally wrap up the particle physics aspects into various forms of inclusive cross sections, where we take a number of integral averages over quantities which are relevant to the particle physics, but less relevant to the transport of particles through the atmosphere. 

As treated by Gaisser in \cite{crapp}, we start by effectively pulling apart the cross sections we can calculate or measure from particle and collider physics (respectively) into the quantity $\langle n^{(b)}_{ac}\rangle$, which is the mean number of particles of type $c$ produced in an interaction between particles of type $a$ and $b$. This is a function of the energy of the interaction between these particles, and from a deeper perspective, a function of the transverse momentum in the interaction, as well as the ratio of the energy deposited into the product to the energy of the primary particle. Such averages exist for different numbers of particles interacting to create different multiplicities of products.

To begin working towards the statistical portion of extensive air showers, in the form of transport equations, we need to finish wrapping the particle physics into convenient quantities. The next step after taking cross sections and turning them into $\langle n^{(b)}_{ac}\rangle$, is to define the quantity $F_{NN}(E,E')$ as:
$$F_{ac}(E_c,E_a)\equiv E_c \frac{dn_c(E_c,E_a)}{dE_c},$$
which is the unit-less inclusive cross section for a particle of species $a$ to interact and produce $dn_c$ particles of species $c$ within an energy bin the size of $dE_c$ around an average energy deposited into the outgoing particle $E_c$ with incident energy $E_a$. This quantity has already been appropriately integrated over transverse momentum. These $F_{ac}$ cross sections are useful in a number of calculations, and it is here where it becomes important to introduce the concept of correlated and uncorrelated fluxes. 

In the history of cosmic ray physics, people have been concerned with particles of energies where air showers are induced and observable, but also with energies where a detection method would never be able to tell whether two particles stemming from the same interaction were related in any way. The former, air showers, are considered a \textit{correlated flux}, where an observer on the ground would be able to tell that the particles they are seeing come from the same primary interaction. However, many interesting phenomena and hints of fundamental physics are contained in the interactions from particles that are not high enough energy to cause correlated fluxes. In these uncorrelated fluxes, we consider the production that a portion of a species' spectrum induces. Each interaction that produces final state observable particles is not important, it is the total spectrum that we observe on the ground that makes the observable quantity in uncorrelated fluxes.

In general, we can extend the treatment covered herein from correlated fluxes to uncorrelated fluxes by invoking the spectrum weighted moment, which effectively tells you how a spectrum of particles of a particular type produce how many particles of another type. To this end, the transport equations for correlated and uncorrelated fluxes are closely related by whether or not we treat $F_{ac}(E_c,E_a)$ via it's own contribution integral in a transport equation, or if we further wrap it into a \textit{spectrum weighted moment}. These moments make what Gaisser calls ``approximation A", which is effectively that all of the relevant quantities, but in particular the unit-less inclusive cross section, do not depend on energy. This is, of course, untrue over large energy scales, but it is easily believable over the energies at which uncorrelated fluxes are relevant.

The spectrum weighted moment is calculated as:
$$ Z_{ac}=\int_0^1 (x_L)^{(\gamma-1)}F_{ac}(x_L) dx_L ,$$
where $x_L=E_c/E_a$, is frequently used in the transport equations we are about to discuss, when extending them to uncorrelated fluxes.

\subsection{Transport Equations}
Once the particle physics has been appropriately handled, the production of particles through the atmosphere is determined by transport equations, which effectively describe the production of particles from each species to the others as a function of depth. For example, a basic transport equation which describes the production of pions from nucleons follows:


\begsp{ \frac{d\Pi}{dX}=-\left(\frac{1}{\lambda_\pi}+\frac{1}{d_\pi}\right)\Pi+&\int_0^1 \frac{\Pi(E/x_L)F_{\pi\pi}(E_\pi,E_\pi/x_L)}{\lambda_\pi(E/x_L)}\frac{dx_L}{x_L^2}\\
 &+\int_0^1 \frac{N(E/x_L)F_{N\pi}(E_\pi,E_\pi/x_L)}{\lambda_\pi(E/x_L)}\frac{dx_L}{x_L^2}.\label{transport}}

Here, we have a number of characters, but the relevant ones for an explanation of transport equations are $\Pi$, the number of pions, and $N$ the number of nucleons, along with the $F_{ac}(E_c,E_a)$ style inclusive cross sections \cite{crapp}. While this is far too simple of a model to describe what happens in UHECR air showers, it stands to illustrate how these calculations can be handled analytically or numerically. The leftmost term on the right hand side of the equation represents the two modes in which the pion would exit the shower, through either an interaction that would effectively take it out of the shower, or that the pion would decay. The middle term represents the possibility that the pion will scatter elastically (in the particle physics sense) and remain a pion, while the right term represents nucleons colliding into anything (but probably a nitrogen molecule, i.e. other nucleons) and producing a pion. 

To extend this system fully, we would write a term for each particle in the shower (after we exit the LHC-level-interactions portion) which can produce pions, and then do the same for each other species of particles. In simple cases, we can compute these integrals and complete a computation of the abundance profiles of various species through the shower. In more complex cases, and perhaps in the most general case, we would apply a numerical solver to the set of differential equations to find such profiles.

An example applied to uncorrelated fluxes, is the electron neutrino production spectrum as measured by the IceCube experiment \cite{atmos}.  Through this process, and the extended process for Kaons, neutrinos are produced:
\begsp{
\label{neuprod}
\pi^{\pm} \rightarrow&\,\, \mu^{\pm}+\bar{\nu}_\mu / \nu_\mu \\
 & \,\,\,\searrow \\
&\,\,\,\,\,\,\,\,\,\,\,\mu^{\pm}\rightarrow e^{\pm}+\bar{\nu}_e/\nu_e+ \nu_\mu /\bar{\nu}_\mu\, \nonumber
} 
for which we can write the muon production spectrum as:
\begeq{\mathcal{P}_\nu=\frac{\epsilon_\pi}{X \cos{\theta}(1-r_\pi)}\int_{E_\nu/(1-r_\pi)}^{\infty}     \frac{\Pi(E,X)}{E}\frac{dE}{E} + \frac{\epsilon_K}{X \cos{\theta}(1-r_K)}\int_{E_\nu/(1-r_K)}^{\infty}     \frac{K(E,X)}{E}\frac{dE}{E}.    }
The objects in this equation are analogous to those in \autoref{transport}, except here we have a term for the production of neutrinos from Kaons. After substituting in the integrals, we can use this to compute an energy spectrum using spectrum weighted moments (\cite{crapp}):
$$
\label{thespec}
\frac{dN_\nu}{dE_\nu}=\frac{N_o(E_\nu)}{1-Z_{NN}} \left(\frac{\mathcal{A}_{\pi\nu}}{1+\mathcal{B}_{\pi \nu}\cos{\theta}E_\nu / \epsilon_\pi} +0.635 \frac{\mathcal{A}_{K\nu}}{1+\mathcal{B}_{K \nu}\cos{\theta}E_\nu / \epsilon_K} \right),         
$$
where,
$$
\mathcal{A}_{i\nu}\equiv Z_{Ni}\frac{(1-r_i)^\gamma}{\gamma+1} \mbox{   and   } \mathcal{B}_{i\nu}\equiv \left(\frac{\gamma+2}{\gamma+1} \right) \left(\frac{1}{1-r_i} \right) \left(\frac{\Lambda_i-\Lambda_N}{\Lambda_i \ln(\Lambda_i/\Lambda_N)} \right).
$$
Where $\gamma$ is the spectral index less one, $r_i$ is the Moli\`{e}re radius of a given species and the $\Lambda$ terms are interactions lengths.
Frequently, a more easy to fit form is used in experimental applications, such as this one from an IceCube report on atmospheric neutrinos \cite{atmos}:
$$
\Phi(E_\nu)=C E_\nu^{-\alpha}(w_\pi+w_K),
$$
That said, only the transport equations which do not use spectrum weighted moments, i.e. those for uncorrelated fluxes, are relevant to the propagation of UHECR extensive air showers in the atmosphere (or elsewhere). While these equations were important when researchers truly started to understand the nature of extensive air showers, we should revisit the fact that today careful and vast Monte Carlo simulations are the primary tool for understanding extensive air showers.
\subsubsection{Grammage, Interaction Length and Radiation Length}
\label{intlen}
After touching on the particle and statical physics processes involved in air showers, we need to discuss how the particles in the air shower interact with the atmosphere. Before discussing specific interactions, we first introduce some important quantities dealing with height in the atmosphere and how particles transport through it. 

How height is measured in UHECR physics may seem counter intuitive to the outside observer. In terms of how particles interact with the atmosphere, the density of the atmosphere is of primary importance, however, the density as a function of physical position (for example, altitude above sea level) can change based on upper atmospheric weather. To account for this, the height in the atmosphere is measured by \textit{vertical depth} (also called grammage, a quantity originally used in papermaking), which is the integrated density of the atmosphere along a line. This quantity can be thought of as how much material one would catch if one dropped a box straight down to a particular height. Deriving further off of arguments presented in Chapter 3 of Gaisser \cite{crapp}, we find this relation:
$$X_v=\int_h^\infty \rho(h)\, dh$$
Now if we take a look at the US Standard Atmosphere of 1976 (\cite{standardatmos}), which is still used today, the density profile (\autoref{density}), is not well described by any easy analytical function, and so we rely on approximations to describe it. 
\begin{figure}[h!]
\begin{center}
\begin{tabular}{c c}
\includegraphics[width=2.8 in]{./images/standard_atmos.png} &
\includegraphics[width=3.1 in]{./images/auger_atmos.png}
\end{tabular}
\begin{singlespace}
\caption[Density in Standard Atmosphere]{Left: Here we have the pressure and density of the standard atmosphere. This could be integrated as a function of height to find the grammage at a given layer of the atmosphere. This comes from the 1976 Standard Atmosphere \cite{standardatmos}. Right: Differences in measured vertical depth at the Auger Observatory from the Standard Atmosphere model, from the Auger Collaboration in \textcite{augeratmos}.}
\label{grammage}
\end{singlespace}
\end{center}
\end{figure}
One such approximation, from \textcite{crapp2}, is plotted in \autoref{grammage}. This figure gives the basic properties of vertical depth, namely that it monotonically increases as a cosmic ray approaches the ground. As shown in \autoref{compositionplot}, the depth of maximum particle production, the best available predictor of primary composition, is measured in grammage. Equipped with this concept, we can begin talking about interaction and decay lengths.

The interaction length takes into account the probability of a species of particle interacting with anything in the atmosphere, and represents the length at which 1-1/e of a population would have interacted. We may also define interaction lengths for different processes, such as brehmstrahlung, Compton scattering of photons, or ionization. These are calculated as:
$$\lambda_j=\frac{\rho}{n_A \sigma_j^{\mbox{air}}}=\frac{A m_p}{\sigma_j^{\mbox{air}}}$$
where $\rho$ is the density of the air, $n_A$ is the number density of nuclei of mean mass $A$, and $m_p$ is the mass of a proton. Note that much of the physically interesting part of the interaction length is contained in the cross section, which is the part we modify if we are concerned with a particular process.

The decay length of a particle that has available states to decay into, commonly muons or neutrons, is written as:
$$d_j=\rho \gamma c \tau_j,$$
with $\gamma$ as the Lorentz factor of the particle, $c$ the speed of light, and $\tau_j$ the particle's decay time. Using this idea of defining lengths over which particles can undergo certain processes, we can continue on to defining radiation parameters relevant to cosmic ray showers.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{c c}
\includegraphics[width=2.8 in]{./images/gaisser_atmos.pdf} &
\includegraphics[width=2.8 in]{./images/inverse_atmos.pdf}
\end{tabular}
\begin{singlespace}
\caption[Vertical Depth and Altitude]{Based on the description and approximation in \textcite{crapp2}, these are some simple plots of vertical depth versus altitude on logarithmic axes, and the inverse on regular axes. These are presented to give some intuitive context to the discussion.}
\label{density}
\end{singlespace}
\end{center}
\end{figure}

\subsection{Atmospheric Losses}
In the energy regime that we are concerned with regarding cosmic ray secondaries ($\approx$100 MeV - TeV), there are two types of losses for charged particles that we must concern ourselves with, namely ionization and brehmstrahlung. Ionization does not scale nearly as strongly as brehmstrahlung. Brehmstrahlung is the basic interaction process of two charged particles interacting and ejecting a photon in the recoil. Cherenkov light is a much lower energy loss process. Both ionization and brehmstrahlung can be accounted for in a simple differential equation from \textcite{crapp2}:
\begeq{\frac{dE}{dX}=-\alpha - \frac{E}{X_0},\label{losseq}}
whose general solution is:
$$E(X)=C e^{-X/X_0}-\alpha X_0.$$
Above, $\alpha$ is the ionization loss in MeV/(g cm$^-2$), and $X$ is the depth traversed. $X_0$ is the radiation length, which is material specific. The left term in \autoref{losseq} represents ionization and approximates no energy dependence in the process while the right term represents brehmstrahlung. The additive term that arrises in the solution is the \textit{cutoff energy}, $E_c=\alpha X_0$.

The analogous and most prominent loss for high energy photons is pair production, through which a photon becomes a positron and an electron, which continue to feed the shower while moving the mean energy per particle down. The probability of pair production in each photon is 7/9 per radiation length. At lower energies, such as medical x-rays, photoelectric losses and Compton scattering dominate \cite{pdg}. 
\subsection{Shower Parameters}%Xmax, LDF, point of first interaction
On its way through the atmosphere, a shower goes through a number of phases, as mentioned in the introduction to this chapter. At the \textit{point of first interaction} the particle undergoes interesting very high energy physics, which can have effects on the shower's dynamics, but on average these effects largely wash out. As the shower leaves it's first couple of interactions, the primary particle species of importance will have been determined, in part at random and in part due to the composition of the primary. The terminology used in the field of extensive air showers (EAS) to refer to the different constiutents of the shower, known as components is:
\begin{itemize}
\item \textit{Electromagnetic Component} (frequently just \textit{EM component} or \textit{EM cascade}): the component of the shower composed of electrons, positrons and photons. Generally, by the time the shower reaches surface detectors such as those in Auger most of the positrons have annihilated in the atmosphere, leading to a net charge in the shower, an essential feature for radio detection \cite{schroeder}.
\item \textit{Muonic Component}: from the initial hadronic interaction at the beginning of the shower, the hard quark-heavy collisions produce charged pions, whose primary decay channel is into muons and neutrinos. This makes the muonic portion of the shower an essential observable for determining the primary's composition. The muonic component effectively serves as a proxy for the number of quarks, and therefore nucleons which initiated the shower.
\item \textit{Hadronic Component}: while it is rarely a direct contributor to the final detectible signal, the hadronic component of the shower, largely composed of pions and some $K_L$ ($K$-long) mesons, is important in some aspects of the shower formation before shower maximum. Interestingly, the largely unobserved hadronic component has been shown to be perhaps the only tell tale sign of Quark-Gluon Plasma in EAS by \textcite{danielle}
\end{itemize}

\subsubsection{Heitler-Matthews Model}
In the commonly used toy model by \textcite{heitler}, and it's extensions such as \cite{matthews}, Particles come from the initial interaction and either pair produce or undergo brehmstrahlung at each ``interaction length", which does not necessarily correspond to an actual interaction length as introduced previously. As these generations continue, they reach the cutoff energy where their losses come primarily from lower energy processes that do not contribute to the shower. This is the same $E_c$ as in \autoref{intlen}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5.9 in]{./images/heitdiag.png}
\caption[Heitler Model Diagram]{Left: a photon-initiated shower in the Heitler model. Right: the extension to hadronic processes by \textcite{matthews} (also the source of this diagram). }
\label{heitdiag}
\end{center}
\end{figure}

In the simple Heitler model, diagrammed in frame (a) of \autoref{heitdiag}, the maximum of particle production, $X_{max}$, is given by (\cite{stanev2}):
$$ X_{max}=\lambda \ln\left(\frac{E_0}{E_c}\right).$$
The classical model does not take into account pionization from hadronic processes, however \textcite{matthews} extends the simple model to account for this, and is often used as a moderately more accurate account of shower physics, which is much more applicable to UHECR physics. To walk through a form from the Heitler-Matthews model \cite{matthews}, we start by assuming the average height of first interaction for a proton initiated shower:
$$X_0=\lambda_I\ln{2},$$
where $\lambda_I$ is the interaction length of the proton, which is a function of energy effectively proportional to $-\ln E_0$. Through arguments about how the hadronic portion of the shower feeds the EM cascade, we can write the production maximum for such a proton shower as:
$$X_{max}=X_0+\lambda_r\ln\left(\frac{E_0}{3 N_{ch} E_c }\right).$$
Here, $N_{ch}$ is the charged pion multiplicity of hadronic production, i.e. how many charged pions are created per high energy hadronic interaction. The three in the denominator comes from the fact that, on average, 1/3 of the produced pions will be neutral, and therefore feed the EM component of the shower. This form can be conveniently rewritten as:
\begeq{ X_{max}=X_{max}^\gamma+X_0-\lambda_r\ln(3 N_{ch}) .\label{matthewsxmax}}
This form isolates the pure EM shower maximum, $X_{max}^\gamma$, from the hadronic contributions, which necessarily decrease the depth of interaction, i.e. hadronic showers will always reach maximum production higher in the atmosphere than an equal energy photonic shower. This form is rewritten with elasticity $K_{el}$ accounted for in \textcite{stanev2} as:
$$X_{max}= X_0 \ln\left(\frac{2(1-K_{el})E_0}{3 N_{ch} E_c}\right). $$
The elasticity is the portion of the energy of the leading hadrons (the particles coming out of the initial very high energy interactions) that gets translated into pions. This effectively lowers $X_{max}$ by stealing energy away from the EM component of the shower and hoarding it until after it is too late to contribute at the shower maximum.

Another important feature of EAS that the Heitler-Matthews model reveals is the \textit{superposition model} or \textit{superposition principle}. Effectively, it is postulated that while Quantum Chromodynamics is extremely difficult to model, we can ignore the inter-nucleon interactions of atomic nuclei primaries and view each nucleon as starting its own shower. In this way, when we consider the showers caused by a particle of atomic mass $A$ and energy $E_0$, we assume, in almost every possible way, that this is the same as a shower consisting of $A$ subshowers of energy $E_0/A$. The modified expression for $X_{max}$, again from \textcite{matthews}, is (with everything else as in \autoref{matthewsxmax}):
$$X_{max}^A=X_{max}-\lambda_r \ln A.$$
This tells us that, as it is in the case of photons to protons, the shower maximum of nuclei should be strictly lower (i.e. higher altitude) than protons, up to statistical fluctuations. A final quantity of historical and intuitive (although perhaps not mechanical) importance to the topic of EAS, is the \textit{elongation rate}. Early in the study of EAS, Linsley introduced this quantity as the base-10 logarithmic derivative of $X_{max}$ with respect to energy \cite{stanev,crapp2,matthews}. Generally, this quantity is used in today's literature to give an intuitive understanding of how much more penetrating a higher energy primary is compared to a lower energy primary, but in the beginning of EAS studies, it was considered to be one of the few quantifiable observable quantities.
\subsubsection{Shower Universality}
\label{universality}
In recent years, the ideas behind \textit{shower universality} have become increasingly common and important. Shower Universality dictates that the \textit{age parameter}, $s$ (a function of vertical depth in the shower), and the lateral distance from the core are the only relevant parameters for the EM component of a shower, up to scaling for energy. It is, in some sense an approximation, and in another a statement, that showers ultimately end up having the same production profiles as a function of depth.  The definition of the age parameter, which will also show up in the discussion about LDFs (\autoref{ldfs}), is \cite{crapp2}:

\begeq{
s=\frac{3}{1+2\frac{X_{max}}{X}\label{ageparam}\,.}
}

Shower Universality is motivated by the observation that shower profiles, in particular electron and photon spectra, are approximately the same at $X_{max}$ for any shower \cite{universality}. This is shaped by the processes of production and absorption in each spectra reaching equilibrium (as they must for it to be a maximum of particle production). 

There is some relatively widespread misunderstanding in the community about the extent to which shower universality is a fact, and to what extent it is a convenient approximation. Of course, it is a mathematical fact that in order to have a maximum of particle production, the competing processes of particle creation and destruction/annihilation (and ultimately removal from the shower), must reach equality at some point \cite{crapp2}.  Since the spectra of EM cascades are shaped by absorption and multiplication, it is in this way shower universality is a fact. That, however, does not guarantee that fluctuations induced by longer-lived hadronic processes can't shape the profile somewhat, and that universality completely neglects the production of muons, a tell-tale sign of primary composition. In these ways, it is a useful approximation, which allows for simple but deep analyses in many situations \cite{universality}.


\subsubsection{Lateral Distribution Functions}
\label{ldfs}
Some detection methods, especially those based on particle detection and counting like water Cherenkov detectors and scintillators only record a shower's footprint on the ground, while others, namely air fluorescence and air Cherenkov give lateral profiles of the shower's development \cite{schroeder}. That said, particle detection methods tend to have very desirable operational qualities, especially near 100\% uptime, lower failure rates, and a lack of moving parts. 

To generally describe the lateral distribution functions (LDFs) used in UHECR physics, they tend to depend on $s$, the age parameter (\autoref{ageparam}), $x=r/r_i$, the distance in Moli\`{e}re radii and some constants, usually related to fluctuations or the total number of particles. A rather complex pair of parameterizations for the electronic and muonic components of the shower are given in \bigcite{crapp2}, based on work in the 60's by Kenneth Greisen amongst others. These are:
\begsp{
\rho_{e^{\pm}}(s,x)&=C_1(s) x^{s-2}\left(1+x\right)^{s-4.5}\left(1+C_2 x^d\right), \\
\rho_{\mu}(r)&=\frac{\Gamma(2.5)}{2 \pi \Gamma(1.25)^2}\left(\frac{1}{320}\right)^{1.25} N_\mu r^{-.75}\left(1+\frac{r}{320}\right)^{-2.5}.
}
The derivation for the muonic component shown above, assumes that no muons are lost in the shower. Counter intuitively, however, in order to adjust this LDF for different heights of the shower, we adjust $N_\mu$ using any number of empirical formulae. Additionally, it is worth noting that the muon LDF cannot, in this form, be adjusted for differently inclined showers. Frequently, before analysis is done using an LDF like this, the shower is effectively ``rotated" so it can be considered vertical. The reconstruction used by the Auger Collaboration uses the ``Auger LDF", which is given by:
%%%% you need to look back t this, ?=a+bsec?
$$S(r)=\tilde C \left(\frac{r}{r_s}\right)^{-\beta}\left(1+\frac{r}{r_s}\right)^{-\beta},$$
where $r_s$ is a characteristic distance used (500m or 1000m in most applications), $\tilde{C}$ is the effective normalization constant and $\beta=a+b\sec\theta$ is a shape factor, which can be used to characterize fluctuations and directional arrival of the shower. 

Such LDFs exist in numerous forms and for numerous applications. One interesting aside, is the current development of Radio detection LDFs. Since radio detection is a relatively young field, LDFs motivated from a theoretical perspective have not been widely developed. In \bigcite{schroeder}, an in depth discussion of this is given. Futhermore, \bigcite{ldfcat} gives a rather excellent account of various LDFs, their parameters, and many other aspects of detection and detector engineering for UHECRs. 
\subsection{Air Shower Monte Carlo Methods: CORSIKA}
\label{corsika}
At time-of-writing, almost all of the aspects of shower and particle physics mentioned in this chapter are, at the professional level, handled by Monte Carlo simulations, and the most consummate package to do so is called CORSIKA \cite{corsika}. This package is used for showers from the low TeV range to the high EeV range and effectively bypasses the need for statistical accounting of species transport by invoking detailed models of particle physics at each step. CORSIKA uniquely takes into account certain aspects of the atmospheric transport and kinematics, but outside of these, relies on invoking the best model of interactions at each level. These include EPOS-LHC, Sybil and QGSJET at the absolute highest energies, down through GHEISHA and FLUKA, and then uses GEANT for its modeling of interactions with matter.

Studies for most major detectors in VHE and UHE astrophysics have been done with CORSIKA. Going into the mid 2000's, CORSIKA cam to dominate the field and is now the gold standard for EAS simulations. Notably, it was used in validation for Auger (\bigcite{corsikauger}) and design work for CTA (\bigcite{ctasika}).
\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.9 in]{./images/corsika.png}
\caption[CORSIKA: Gamma Rays Versus Hadrons]{Left: A gamma ray shower of 100 GeV, longitudinal profile above, lateral profile below. Right: A proton shower of the same energy and same cross sections in each panel. Note the spread of the hadronic shower is much larger, and if you zoom in more closely, you can see the blue hadron and green muon tracks in addition to the red EM component. These images were published in \bigcite{steinke}, but are originally from Fabian Schmidt.}
\label{corsikadiag}
\end{center}
\end{figure}


\newpage
\section{Detection Methods}
\label{detectors}
Over the years since Pierre Auger began studying strange torrents of particles from the sky, many different techniques have been used to detect and identify extensive air showers with ground based techniques. Some of the first versions of these are covered in \autoref{history}, while the section below will exclusively cover techniques used successfully by modern experiments. We start by discussing the techniques of greatest importance to the work done for this dissertation: water Cherenkov and scintillation detection. The discussion will then move into the fluorescence technique for air shower detection, given its importance to surface detector energy estimation. We will then discuss air and ice Cherenkov to give context to multi-messenger neutrino detections, to better methods of air shower composition measurements and to set the stage for a discussion of Cherenkov telescopes. Finally, we will discuss radio detection methods, since the AugerPrime upgrade will be adding radio detectors to each station. There are a number of techniques currently under development, including radar imaging of the plasma disk left after the first interaction and molecular fluorescence, but for the sake of brevity, discussions of these will be left out of this work.
\subsection{Water Cherenkov}
\label{wcd}
Detection of the Cherenkov light made by particles traversing water at speeds greater than the speed of light in the medium is a technique only a couple of decades older than particle physics itself. The first use of Cherenkov light for detecting extensive air showers was by the team led by N.A. Porter at the Harwell Laboratory in England as part of the Atomic Energy Research Establishment (one of the many advances that came out of the greatly increased research funding of the early '50s \cite{scifund1,firstcher}). He and his team were the first to figure out how to stop bacterial and fungal growth long enough to create a stable and viable detector \cite{firstcher}. 

This technique allowed for measurement of a direct proxy of the showers total energy at a point making it a very favorable technique, not to mention that water is much cheaper than a comparable volume of scintillator, and lends itself to detectors with a wide aperture for many zenith angles. It was also N.A. Porter who figured out the fruitful application of highly reflective liners to keep a maximal amount of light in the detection volume, where it might scatter into the photomultiplier tube for detection.

The Haverah Park team used Water Cherenkov detectors in their first proper air shower measurement application, which then served as a stepping stone on the way to deploying the Auger Observatory. As documented in \bigcite{firstcher}, many of the techniques that Porter found effective for Water Cherenkov Detectors (WCDs) were pivotal to the decision to use them in Auger.

Physically, Cherenkov light arrises from the interactions of fast particles with their medium. As mentioned above, the naive explanation for Cherenkov light, is that it occurs when a charged particle is traveling faster than the speed of light in the medium it is traversing. Many texts and educators like to make the analogy that it is like the shockwave which causes sonic booms in air, and although this analogy is useful, it is truly completely different dynamics which govern these physical processes. 

As a charged particle moves through water at any speed, its interactions with the molecules cause slight changes in the polarization of the medium, however when the phase velocity of the light is greater than that allowed by the medium, the medium emits light in spherical shells. This light, however, destructively interferes within what would be referred to as the particles ``Mach cone" and, from a naive point of view, constructively interferes in the forward facing Cherenkov cone. In depth discussion of this is given in chapter 23 of \bigcite{zangwill} and a diagram of some of the relevant quantities is shown in \autoref{cherlight}. The angle of the Cherenkov cone in the laboratory frame is:
\begeq{
\cos\theta_c=\frac{1}{n \beta}\approx\frac{1}{n}
}
\begin{figure}[h!]
\begin{center}
\includegraphics[width=4.0 in]{./images/cherenkov_diagram.png}
\caption[Cherenkov Light Diagram]{A diagram of some of the relevant quantities for Cherenkov light from \cite{cherbaki}. Note that the length of the chord AB divided by the length of AC gives us the cosine of the Cherenkov angle. In this diagram, $\theta$ is the aforementioned $\theta_c$.}
\label{cherlight}
\end{center}
\end{figure}
%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=3.5 in]{./images/cherlight.png}
%\caption[Cherenkov Light Diagram]{Pictured here is a diagram from \bigcite{zangwill} showing some of the relevant quantities in computing the fields for Cherenkov light emission. Particularly prominent here is $\theta_c$ and the Mach cone, which is the complementary angle to $\theta_c$. The $v$ is the velocity of the particle and the circles represent the spherical emissions of the medium.}
%\label{cherlight}
%\end{center}
%\end{figure}
We make the above approximation as the particles we are discussing, even at low energies compared to the scale of UHECR showers, are moving at very nearly the speed of light\footnote{Of course, for some phenomena how close their speed is to $c$ is important, here it is not.}. This emission angle can be very small in the laboratory frame, but luckily for the case of WCDs, this light is emitted inside the detector enclosure and reflected upwards by the liner (the air Cherenkov technique is not so lucky in this regime). This Cherenkov emission rate is constant as the particle traverses the detector and so longer traversals will leave larger signals to be seen by the photomultiplier tubes.

The Auger WCD is diagrammed in \autoref{augerwcd} and with a radius of 1.6m on the top and a height of 1.2m on the sides, it holds about 12 metric tons of water. The most successful technique that has been used to purify the water in Auger tanks is simply to start with relatively pure water, let the microbes and fungi grow in it, and then wait until they have exhausted their food supply. In a light tight tank, there will be no algae or otherwise to fuel an ecosystem and therefore the ``gunk'' will eventually die, falling to the bottom harmlessly. 

Finally, before moving on to scintillation technologies, we must dwell on an important technical point around calibration of a WCD. In Auger, we use a unit called VEM or Vertical Equivalent Muons as a standard measure of the signal produced in an air shower. By making a histogram of pulse heights over many events, one can determine the minimum energy deposited by a muon, which corresponds to it going through the shortest possible path in the detector, or vertically. 
\subsection{Scintillation}
\label{scints}
Scintillation counters or detectors, frequently referred to simply as scintillators, are perhaps the most venerable and longest lasting detection technique. In early experiments, going all the way back to \bigcite{muon} and earlier, scintillators were used to trigger the cameras researchers used to photograph the events they were detecting. The first prominent use of scintillators for very high energy air shower detection was by Linsley at Volcano Ranch, where they were used in much they same way they currently are by a number of experiments. 

Scintillators are usually made of crystals or plastics, and are luminescent, or emit visible light, particularly when exposed to ionizing charged particles \cite{scints}. The scintillators that are useful for particle detection (versus, say, medical or metrology applications) operate on the principles of fluorescence. When a charged particle excites the scintillator, the molecules in the material enter a very shortly held metastable state which is usually lower energy than the initial excitation, and then decay releasing a photon of a wavelength outside of the materials absorption bands. This allows light created by the scintillator to travel through it without having to worry about unnecessary attenuation from processes like the photoelectric effect, which would be the dominant loss in this regime. The minimum energy of the excitation determines the Minimum Ionizing Particle or MIP energy, which is the correct analog of VEM in water Cherenkov.

While scintillators have a number of nice properties, perhaps the best of these for astrophysics is that they are compact and relatively robust, while maintaining the ability to distinguish individual particles under the right conditions. On the other hand, most scintillators degrade over time, and should not be exposed directly to ambient light if avoidable. They also, depending on their for factor, are prone to internal cracking.

Organic scintillators, which AGASA, TA and AugerPrime all use, have short nanosecond or sub-nanosecond response times to though-going particles \cite{augerscint,tadet,scints}. They are extruded from what is, in effect, appropriately doped styrofoam \cite{fermiscint}. Due to their extrusion process, they can be manufactured in a number of form factors. Additionally, both inorganic and organic scintillators come in liquid forms, and can be as simple as chemically simple as liquid Xenon (e.g. direct detection of dark matter experiments).

\begin{figure}[h!]
\begin{center}
\includegraphics[width=2 in]{./images/primescint.png}
\includegraphics[width=3.6 in]{./images/tascint.png}
\caption[Auger and TA Scintillators]{Left: A photo of the internal fiber structure and scintillator panels of the SSD for the AugerPrime Upgrade from \bigcite{toprime}. Right: A diagram of the two layer structure of the TA surface detector's scintillator panels, from Abu-Zayyad et al., 2012 \cite{tadet}.}
\label{scintdiag}
\end{center}
\end{figure}

In the AugerPrime implementation, many of the same principles as water Cherenkov are used in the design of the scintillator housing. In particular, a highly reflective titanium oxide layer is used to coat the sides of the scintillator panels, which allows reflection of pulses increasing the fairly short ($\approx$10 cm) attenuation length of the material. Birefringent wavelength shifting optical fibers are used to transport the light around the edges of the panel to the photomultiplier tube. The detectors are to be called SSDs or Scintillator Surface Detectors, and will consist of 4 m$^2$ of aperture atop each of the 1660 stations in the array \cite{augerscint}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.9 in]{./images/prime3d.png}
\includegraphics[width=2.7 in]{./images/primepic.png}
\caption[AugerPrime Stations]{Left: a 3D mock-up of the AugerPrime station with scintillators prominently featured on top, from \cite{toprime}. Right: A picture of the first SSD in the field from \cite{augerscint}.}
\label{primepics}
\end{center}
\end{figure}
\begin{singlespace}
\subsubsection{Comparison and Compatibility of Water Cherenkov and Scintillation Detectors}
\end{singlespace}

Scintillators and water Cherenkov detectors (WCDs) operate at the highest level in very similar ways. The detector volume, either the water or the plastic scintillator, is connected to a PMT (for the scintillator, it is usually mated using an optical glue or grease, and for the water it is partially submerged). While scintillators do not carry the issues of water purity and PMT suspension, they are more expensive and fragile. Additionally, they are relatively two dimensional thin sheets and often require a lead layer on top to encourage pair production from the photonic portion of an incoming extensive air shower. WCDs, on the other hand, have a favorable aperture vs. zenith profile, allowing faithful detection and reconstruction of high zenith angle showers (\autoref{scintapp} shows a plot of the WCD from Auger and the SD from TA's apertures). In defense of scintillators, the deployment process for WCDs frequently involved trucks of purified water, flatbeds which can only carry two or three stations and sometimes even helicopters. Logistically, WCDs can be challenging and expensive.

Perhaps \autoref{scintapp} may appear as a relatively unfortunate fact for scintillators, but looking forward, there is a good argument to be made that radio detection and scintillation detection are complimentary techniques. As the radio detection technique advances, upward facing scintillators with high aperture to vertical showers, and radio antennae with high aperture to inclined showers (more on this in \autoref{radio}) provide similar measurements, i.e. high EM and low muon sensitivity. In this way, perhaps the best detector one could build would utilize all three of these techniques to accurately determine composition with high aperture at all angles and relatively low cost for the performance. Modern electronics have made handling this data rate possible and the only seriously needed technology is cheap radio repeaters, which exist but have not been readily configured for use in air shower arrays.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4.2 in]{./images/applot.pdf}
\caption[Auger and TA Surface Detector Apertures]{Here is a comparison of the aperture as a function of zenith angle for the Auger and TA surface stations. Water Cherenkov detectors tend to allow higher apertures for lower cost, but are logistically inconvenient.}
\label{scintapp}
\end{center}
\end{figure}

Historically, water Cherenkov and scintillation detectors have not been used in concert. However, in considering ways to better measure the composition of UHECR primaries, the Auger Collaboration has decided to pursue the use of both detector types in tandem. This allows one to use the different muon and electron efficiencies of water and scintillator detectors to determine the number of muons and electrons that hit a station during a shower (see \autoref{showercomp} for a more in depth discussion of this). As mentioned previously, it is this quantity, $N_\mu/N_e$ that is perhaps the best estimator of composition that is available to surface detectors.


%include zenith vs. aperture plots made with Sean
\subsection{Atmospheric Fluorescence}
\label{fluor}
Originally pioneered by the Fly's Eye experiment (in conjunction with Volcano Ranch), the very successful Nitrogen Fluorescence technique provides perhaps the most visually relatable detection of UHECR showers. Besides Fly's Eye and its successors, Auger and TA also operate fluorescence telescopes \cite{ultraray,flyseye}. 

As charged particles from an air shower traverse the atmosphere, they excite the metastable states of the Nitrogen atom, which in turn emit light in the visible spectrum. The key detail of this process, and the fact which makes the fluorescence technique successful is that the emission of this light is isotropic. Because of this, an array of telescopes can be set up to view a fiducial volume of atmosphere. 
\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.8 in]{./images/flourdiag.png}
\includegraphics[width=2.8 in]{./images/flourtrack.png}
\caption[Fluorescence Reconstruction]{Left: A diagram of the principle at work in FD reconstruction. This is originally from the doctoral thesis of a student of Greisen named Brunner, reproduced in \cite{ultraray}. Right: A reconstructed track from the Auger FD, taken from \cite{fluorauger}.}
\label{fluorpics}
\end{center}
\end{figure}
Since the amount of fluorescence is effectively proportional to the high energy part of the EM component of the shower, the fluorescence technique allows for direct observation of $X_{max}$. In fact this was one of the driving forces behind the construction of the Fluorescence Detector (FD) at Auger; accurate measurements of $X_{max}$ by the FD gives the approximate composition of primaries (averaged over many showers), while the 100\% uptime of the WCDs fills out the spectrum. 

Technically, fluorescence telescopes work on a very basic reflector design. Usually a large mirror of some sort focuses the light onto the focal plane, which is packed photomultiplier tubes. These are almost universally in a hexagonal tiling \cite{flyseye}. In modern applications, many times the primary mirror of the telescope is segmented \cite{fluorauger}. This allows for greater field of view and more light collection, while avoiding many of the complications of segmented telescopes since the PMT pixels are so large the point spread function (PSF) of the telescope does not have to be nearly ideal. Accordingly, no adaptive optics or other corrections are needed. 

A pitfall of fluorescence telescopes is that, much like optical telescopes, they can only be operated on nights with a clear moonless sky. This brings their typical duty cycle down to the range of 5-10\% \cite{flyseye,fluorauger,ultraray}. While this sounds bleak, the lack of exposure from time is made up for with the increased instrumented volume. Furthermore, the aperture of the FD array is a function of the energy of showers. In the Auger FDs, low energy showers are not even collected throughout the entire array due to insufficient light, however $10^{19}$+ eV showers are collected even outside of the array in some cases. At $10^{17.5}$ eV we have 900km$^2$sr and at $10^{19}$ eV, we have 7400km$^2$sr of aperture \cite{fluorauger}.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.8 in]{./images/flyseyefd.png}
\includegraphics[width=3.0 in]{./images/augerfd.png}
\caption[Auger and Fly's Eye FDs]{Left: A telescope from the original Fly's Eye detector, published in \textcite{flyseye}. In the center, we have the camera module contained in the stainless steel drum. Right: One of the Auger Fluorescence detectors. In the middle of the picture between the aperture on the left and mirror on the right is the honeycomb lattice of PMTs.}
\label{fdpics}
\end{center}
\end{figure}
\subsection{Direct and Ice Cherenkov}
As we discussed in the Water Cherenkov section, Cherenkov light is emitted any time a charged particle traverses a medium faster than the local speed of light. Since the particles in an air shower are all effectively moving at the speed of light, we need not wait for the shower to hit a relatively high index of refraction medium like water, we can detect the Cherenkov light produced from the air. I have also put Ice Cherenkov in this category since, from a hardware point of view, they are very similar techniques.

The technique was first pioneered in the same era as N.A. Porters work at the Harwell laboratory. John Jelley and Bill Galbraith went off of work by P.M.S. Blackett, a Nobel prize winning cosmic ray physicist, and looked for pulses from the night sky with an oscilloscope. Over time, their experiments became more complex and the eventually proved conclusively that they were detecting Cherenkov light produced in air. Interestingly, Blackett was convinced that we would be able to lay back on a dark night and view the Cherenkov pulses with his bare eyes \cite{firstcher}. The technique was later used with mild success in air shower physics.

Ultimately, the first very successful application of air Cherenkov was in Imaging Atmospheric Cherenkov Telescopes. In this configuration, gamma rays enter the atmosphere and interact at relatively high vertical depth (i.e. low altitude). They produce almost exclusively electromagnetic showers (as seen in \autoref{corsikadiag}), wherein the electrons, which have a relatively low threshold for producing Cherenkov light in air, create a large and relatively bright cone which hits the telescopes optics and is then focused into an array of photomultiplier tubes. Some of the experiments that make use of this are VERITAS, MAGIC and HESS. The original Whipple observatory operated into the new millennium as well. These experiments will be superseded by the Cherenkov Telescope Array, or CTA, in the next decade.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=4.5 in]{./images/gammacomp.png}
\caption[Comparison of UHECR and VHE Gamma Ray Detectors]{This diagram from \textcite{ultraray} shows the basics of the IACT method of gamma ray detection in comparison to the standard method of UHECR detection. To be fair to HAWC, Milagro and CASA-MIA, there are a number of surface detector arrays for high energy gamma rays.}
\label{gammacomp}
\end{center}
\end{figure}

In UHECR physics, a number of experiments have toyed with the idea of directly detecting the Cherenkov radiation from extensive air showers. The issue with this technique, and the reason it is not more widely adopted, is firstly that the Cherenkov cone from vertical showers is relatively small (\textless a few degrees), while the Cherenkov light from oblique showers is usually too attenuated to be detected. Furthermore, this technique effectively involved exposing bare or lightly filtered photomultiplier tubes to the sky. This tends to be a fairly risky proposition since weatherproofing is non-trivial and requires moving parts to allow for remote operation. On top of all this, these sky facing PMTs can only be operated on clear moonless nights\footnote{Much of this discussion is informed by \textcite{schroeder}, which serves as an excellent review of all of UHECR physics, and not just radio detection.}. 

One benefit of the direct air Cherenkov technique, is that an array of this style would be able to faithfully distinguish the $Z$ number of the primary to $\approx$5\% as long as it has a small enough time resolution (\textless 2 ns) \cite{swordyair}. This type of experiment has been deployed, although the only one currently operating is TAIGA (formerly Tunka-133). Briefly, it is an array of upward facing photomultiplier tubes that view the night sky when they can.

The form of this technique used by IceCube, which could be described as an ice Cherenkov experiment, takes strings of photomultiplier tubes with onboard electronics more than 1 km long and immerses them in the Antarctic ice. The tubes face down towards the center of the Earth (a useful quality for background rejection) since the neutrinos they are looking for are more likely to interact on their way through the Earth than through the atmosphere. In a naive sense, IceCube is an array of Tunka-133s in layers, facing downward. Since the index of refraction is higher in the ice, the particles excited and created by the neutrino interactions in the ice will produce wider Cherenkov cones and therefore it will make detection easier.

\subsection{Radio}
\label{radio}
While the possibility of detecting air showers via radio antennae had existed at least by the 90's (it was considered as the technique for the Auger surface detectors \cite{firstcher}), this method of detecting UHECR showers has really come to fruition in the last decade. At time of writing, a number of experiments including LOFAR, KASCADE-Grande, Auger and IceTop (IceCube's surface detector) are adding radio components to their detectors \cite{schroeder}. Much of this work as it pertains to UHECRs was initially done in the Auger Engineering Radio Array (AERA) \cite{radioaera}. 
\begin{figure}[h!]
\begin{center}
\includegraphics[width=4.5 in]{./images/geoask.png}
\caption[Radio Emission Diagram]{This diagram shows, in the top left, the kinematics of the geomagnetic effect and under it the polarization. On the right it shows the Askaryan effect and below it, the radial polarization it produces. Sourced from the excellent review by \textcite{schroeder}.}
\label{rademission}
\end{center}
\end{figure}
The physical basis of the technique arises from two effects, the geomagnetic effect and the Askaryan effect. As the shower progresses, the large numbers of positively charged and negatively charged electrons are separated over large distances by Earth's magnetic field. This phenomena of charge separating is the same principle as the operation of a simple Hertz dipole antenna, however it has no limit to how separated it will become. This means that a wave packet of increasing wavelength per time is created. This effect is detectable in and of itself so long as the shower is perpendicular enough to the geomagnetic field and creates linearly polarized light. 


Joining in concert with the geomagnetic effect, the Askaryan effect arrises from positron depletion in the shower. The essence of the Askaryan effect is positrons annihilating in the medium as the shower progresses. This leaves an excess of electrons in the shower and as the total charge of the shower changes ($\dot{Q}$), they create radially polarized waves in radio wavelengths. In solid matter, the Askaryan effect dominates dramatically over the geomagnetic effect, while in air the lower density makes it such that the Askaryan effect is still prominent but not as important as the geomagnetic effect \cite{schroeder}. A diagram of both effects is shown in \autoref{rademission}.

Similarly to air Cherenkov, radio only emits within and near the edges of the Cherenkov cone of the shower. The technique is saved in the UHECR regime (in contrast to air Cherenkov), because of the geometrically expanding cone of highly inclined showers. In other words, for inclined showers the longer time of propagation allows the Cherenkov cone to spread, therefore making it detectable by multiple Auger stations. This, coupled with the radio-transparency of the atmosphere, has made radio detection a fairly successful technique.
%there's more I could say here, but im moving on for now
\newpage
\section{Operating UHECR Experiments}

Here we give an overview of currently operating experiments relevant to this work. We will discuss each of Auger, TA and CTA with importance placed on describing Auger and AugerPrime, the central foci of this work. The discussion of CTA paves the way for \autoref{timatcta}, while the description of TA corresponds to \autoref{augeratta} and the description of Auger begins the work described in \autoref{electronics} and \autoref{augtiming}.


\subsection{Pierre Auger Observatory}
\label{auger}
\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.0 in]{./images/augerdotmap.png}
\includegraphics[width=3.3 in]{./images/augerdetmap.png}
\caption[Maps of Auger]{Left: A dot map of my own creation showing the positions of the surface detectors. Note that some areas are not instrumented due to terrain. This dot map makes the position of the infill and engineering arrays obvious by the bunchin towards the top left. Right: The standard mapping diagram used by the collaboration, showing the FDs and SD positions as well as local geography, from \cite{auger2015}.}
\label{augmaps}
\end{center}
\end{figure}
The Pierre Auger Observatory is a large UHECR observatory in Malarg\"{u}e, Mendoza Province, Argentina. It consists of 1660 surface detectors and 24 Fluorescence detectors instrumenting an area of \textgreater 3000 km$^2$ (maps presented in \autoref{augmaps}). The fluorescence telescopes have a duty cycle of about 15\%, while the surface detectors have \textgreater 99\% uptime, interrupted primarily by solar power outages from the rainy season. A rather large spacing of 1.5 km between stations in a hexagonal grid ensures maximum aperture in the desired energy range of 10$^{18.5}$+ eV. From the very large instrumented area and near 100\% uptime of the array, the observatory has a total exposure of 40,000 km$^2$ sr yr \cite{auger2015}. This makes the observatory the largest and highest exposure in the world.

Design began in 1995 after the idea was conceived by Jim Cronin and Alan Watson. Prototyping ran until 2002 when construction officially began. This ran until 2008, when the full array was commissioned, however the array started taking data in 2004. While a number of designs for the surface detectors were considered, including water Cherenkov, scintillation, RPCs or radio detectors, 
ultimately the water Cherenkov technique was found to be the best in terms of cost per aperture per angle. 

\subsubsection{Auger Surface Detector}

\figwrap{This is a digram of the housing that the PMT is situated in to be partially immersed (i.e. optically coupled) into the water in the WCD. Figure from \textcite{auger2015}.}{./images/pmthouse.png}{2.5 in}{r}{Housing of Auger PMT}{pmthouse}

The Auger Surface Detector consists of a large tank of about 12 metric tons of water, 3.6 m in diameter and 1.2 m tall (1.2 meters is chosen as it is optimal for Cherenkov production versus attenuation), made of polyethylene with a Tyvek reflective liner. It has 3 photomultiplier tubes mounted on top, each housed in an air tight container (\autoref{pmthouse}), slightly submerged in the water thereby optically coupling them to their detection volume. The photomultiplier tubes are 9" with eight dynodes (model Photonis XP1805/D1). They have their preamplifiers and high voltage supplies built into their base. 
\begin{figure}[h!]
\begin{center}
\includegraphics[width=5.5 in]{./images/augerwcd.png}
\caption[Auger Surface Station Diagram]{Above is a schematic diagram of the WCDs used by (left) N.A. Porter, the leader of the team that created the water Cherenkov technique, and (right) the WCD used by Auger. In some sense, the Auger WCD is the grandchild of the Porter version. From \textcite{ultraray}.}
\label{augerwcd}
\end{center}
\end{figure}
Digging in to the technical details, photomultiplier tubes are sampled at two different gains by a 40 Mhz, 10 bit flash Analog-to-Digital Converter (ADC, Analog Devices model AD9203). Their output is fed into either two Altera ACEX FPGAs, or one Altera Cyclone, depending on the date of construction. Later models have the single FPGA; there is also a block ram that is moved into the Cyclone in the newer version. Trigger logic is handled in the FPGA, and the appropriate outputs are made available from there to feed data into the onboard computer. The processing system is a IBM PowerPC 403 GCX at a clock rate of 80 MHz with 32MB of RAM, 2MB firmware storage.

For timing, the stations employ a custom ASIC with a 100Mhz counter which takes in the sawtooth granularity correction from the GPS units. The model chosen for the GPS timing receiver is the Motorola Oncore UT+ which features a 20Mhz clock capable of 25 ns RMS accuracy with the clock granularity message (sometimes called the sawtooth, after the shape it makes over time). 

Communications is handled by a custom radio from the University of Leeds electrical engineering team which runs in the hundreds of bits per second range. Power is handled by a solar panel and lead acid batteries. Once the stations are set up, they are fairly robust and the main sources of maintenance are standard electronics errors, weather effects (in some situations, the SD is the tallest metal object in a large area, which is to say lightning strikes do destroy them sometimes), and the occasional issue with the local goats climbing onto the top of the tanks and knocking over solar panels and antennae.


\subsubsection{Auger Fluorescence Detector}
Some of the details of the FD has already been discussed in \autoref{fluor}, but I will reiterate and expand on these here. The collection of 4 sites with 6 telescopes each are usually collectively referred to as the Fluorescence Detector, although sometimes the term FDs is used to refer to the individual telescopes. Each telescope has a $30\degree\times 30\degree$ field of view, and each station with 6 telescopes then has a 180\degree field of view around it. 

The telescopes are equipped with rectangular aperture mirrors, with a set of corrective lenses in front of them. Although the mirror's diameter is 3.4 m, the effective aperture is close to 1.5 m diameter due to the geometry of the housing \cite{fddiam}, shown in \autoref{augfd}. Each telescope is equipped with a garage-door style shutter, which is operated automatically, but monitored at night to ensure faithful operation, which is shown open for maintenance in \autoref{augfd}. There is also a backup cloth shutter that can be employed in case of emergency. Most weather issues are also handled automatically, and the corrective lenses and filters at the aperture work as a window to keep the cool air from the air conditioners inside. Precautions are taken to keep the facility clean as well \cite{auger2015}. 
\begin{figure}[h!]
\begin{center}
\includegraphics[width=2.5 in]{./images/fddiag2.png}
\includegraphics[width=3.0 in]{./images/fdstation.png}
\caption[Auger Fluorescence Detector]{Left: A diagrammatic representation of the components of the fluorescence detector. Right: A photo of the fluorescence detectors with their shutters open for maintenance. Both from \cite{auger2015}.}
\label{augfd}
\end{center}
\end{figure}
Scientifically, the FD works as a very important calibration component for the SD, as well as the science that it accomplishes alone. The events caught by the FD and SD simultaneously are called ``golden hybrids'' and are used to determine the correct SD energy estimators to use. This is due to the fact that, at time of writing, the errors and energy calibration of fluorescence detectors are the best understood amongst all of the UHECR detection techniques \cite{schroeder}. In this way, the energy is determined from the SD and from the FD and then used to recalibrate the energy estimators used for the SD.

One rather interesting aspect of the FD's construction, is that each station (i.e. each 6 telescope bundle) is controlled by a single computer called an \textit{EyePC}. The readout electronics for each telescope are connected via firewire to what is called an MPC. These MPCs are basically single board computers which bring the firewire data in, buffer it, and then send it across a Base-100 ethernet connection to the EyePC \cite{auger2015}.
\subsubsection{AugerPrime Upgrade}
AugerPrime, an upgrade to Auger that is currently underway and the main focus of the original work in this document, will provide new electronics to each of the 1660 stations. This will include new ADCs, new trigger processing, a new central FPGA and processing system on the same chip and a number of other upgrades. These will be discussed in depth in \autoref{electronics}. In finality, the upgrade will add radio and scintillation detectors to each station in addition to the improved electronics.
\subsection{TA}
\label{ta}
Near Delta, Utah (not far from the Dugway Proving Grounds, the historic site of CASA-MIA and Fly's Eye), the former AGASA group operates the Telescope Array (TA), a detector which is similar in nature to Auger. Schematically, it has FD and SD components which can operate in hybrid mode, like auger. There are a handful of differences and we will summarize a few key points below \cite{tale}.
\begin{itemize}
\item TA uses double-stacked scintillators, while Auger uses water Cherenkov detectors.
\item Auger views the southern hemisphere's sky; TA views the northern hemisphere's sky.
\item Outside of trigger processing, almost all in-situ analysis for the Auger SD is done on the processing system, while TA moves much of this to the FPGA. This gives faster event registration, but lends itself to operational opacity.
\item TA employs a more rigorous data-blinding process, while Auger data are available to collaboration members on a daily basis.
\item Auger does not section its surface array, meaning that all coincidences of any subset of stations will be recorded, while TA sections its array (see \autoref{tadiag}) into three parts, making data collection easier but preventing a small number of detections on the edges of the sections.
\end{itemize}
\begin{figure}[h!]
\begin{center}
\includegraphics[width=3.4 in]{./images/tamap.png}
\includegraphics[width=2.4 in]{./images/tasd.png}
\caption[TA Map and Surface Detector]{Left: A map of the telescope array located near Delta, Utah. Right: The TA surface detector with components diagrammed. Both from \cite{tasd}.}
\label{tadiag}
\end{center}
\end{figure}
The surface detector for TA uses 507 stations with a 1.2 km hexagonal spacing. As with Auger, these detectors have 100\% uptime and are used to measure the spectrum with relatively high statistics \cite{tasd}. These detectors instrument a total area of 700 km$^2$, of which the area covered by their FD is a subset. The electronics consist of a CPU board which is fed data by an FPGA. The CPU is a Renesas SH-4 running at 266Mhz, which is interestingly the same unit that powers the 1998 Sega Dreamcast. The programmable logic is provided by a Xilinx SPARTAN3 FPGA, and the FADC is a 50 MHz 12-bit Analog Devices AD9235.


\subsubsection{Work Towards a Cross Calibration of Auger and TA}
\label{augertamotiv}
There are a number of unresolved questions in the field of UHECR physics, and a handful of these revolve around tensions between the data collected by Auger and the data collected by TA. In particular, Auger has shown that in its spectrum, as the energy increases, the composition gets heavier \cite{heavycomp}. TA reports the opposite. Both collaborations use fluorescence measurements as the standard of measuring showers, although Auger is uniquely equipped to measure composition with the SD array. Going forward, the AugerPrime upgrade will allow a much more accurate determination of composition from Auger's SD array.

Additionally, TA was originally built to answer a question that haunted Auger, which is the issue of different energies as reconstructed by the FD and SD. Fluorescence techniques are considered to be powerful enough that the SD at Auger is calibrated based off measurements made with it, however when the SD reconstruction is done from first principles, it does not agree with the FD reconstruction \cite{tale}.

In an attempt to resolve these tensions, the High Energy Astrophysics group at CWRU along with the Auger group at the Colorado School of Mines have undertaken the task of setting up an Auger WCD in the TA facility for direct cross calibration \cite{sean}. Ultimately, this involved preparing most of the equipment to run a detector of our own design in the TA, since the TA stations do not have accessible local triggers. 


\begin{figure}[h!]
\begin{center}
\includegraphics[width=4.5 in]{./images/augerta.png}
\caption[Auger@TA Setup]{Here are two photos of the Auger@TA site with components labelled. From \textcite{sean}.}
\label{augta}
\end{center}
\end{figure}

In all, our detector, an Auger WCD of the same design deployed in Argentina, and Auger North WCD (a prototype for a cancelled project) and two TA detectors are running at the Extreme Laser Facility (panoramas shown in \autoref{augta}). One of the TA stations reads out global TA triggers, while the other is effectively used for it's detection electronics and hooked up to all custom processing, including discriminators, coincidence gates and time tagging electronics. The construction of the time tagging electronics will be discussed in \autoref{augeratta}.
%\subsection{Ice Cube}
%\label{icecube}
%Deep under the antarctic ice, the IceCube collaboration has constructed 1km$^3$ detector detector, aiming at searching for neutrinos over a wide range of energies. Neutrino detectors must be designed around the fact that, while there are a lot of neutrinos, they have a very low probability to interact in a given volume. That probability to interact, just like charged particles and photons, is still related to the density of the medium they are traversing, but in the case of neutrinos, one cannot guarantee an interaction even after passing through the Earth. 
%
%The construction of IceCube reflects the neutrinos' improbability of interaction by employing 3.5km long cables to sink 60 detectors, or Digital Optical Modules (DOMs) 1.5km below the surface of the ice antarctic. These DOMs face downward, looking for upward going neutrino interactions (although IceCube is still sensitive to downward going showers as well). Each string of detectors is spaced 125m from the closest other strings in a hexagonal configuration. This leads to a total instrumented area of 1km$^3$, although they are sensitive to events that occur outside the detector as well, as long as enough of the shower makes it into their detector \cite{icebasic}.
%
%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=2.8 in]{./images/icyboi.png}
%\includegraphics[width=2.8 in]{./images/icedom.png}
%\caption[IceCube Diagram and DOM]{Left: A 3D rendering of the design of IceCube. This schematically shows how deep the strings go. Right: The Digital Optical Module rendered with parts diagrammed. While it does not look like an Auger Tank, it is effectively performing the same functions as one. Both taken from \cite{icedom}.}
%\label{icediag}
%\end{center}
%\end{figure}
%IceCube's predecessor, AMANDA  (Antarctic Muon And Neutrino Detector Array), worked on many of the engineering issues with IceCube. The most prominent of these in terms of detector electronics, was that the impedance of the long cables used to readout the detectors would mar the pulse width, extending it from a handful of nanoseconds to milliseconds. Reversing this pulse shaping could get you an accurate arrival time, but not individual photoelectron pulses. Using optical signals was possible, but the optical transmission technology at the time did not allow for a high enough dynamic range in analog pulses \cite{icebasic}. Ultimately, both methods pointed to the need for a detector of this type to digitize its pulses in-situ. 
%
%Interestingly, the DOM technology for IceCube is parallel in development and conceptually close in operation to an Auger WCD. Once a DOM is frozen into the antarctic ice, it cannot practically be retrieved, and so it must have a very low failure rate (90\% survivability over 15 years is the target, 94\% is the current measured rate) \cite{icedom}. The DOM must perform all of its calibrations in-situ less timing calibration. 
%
%The timing system for IceCube consists of a number of DOMHubs, which use custom timing boards to distribute GPS disciplined timing signals throughout the array. Each DOM's timing is calibrated in a call and response fashion through what they refer to as RAPCal, or Reciprocal Active Pulsing calibration. This precise timing calibration helps DOMs with local triggers, which are passed up and down the cables to neighboring detectors to determine if a 1/4 photoelectron trigger is noise or an event. The DOM's electronics include a type of flash ADC and two digitizers. The two digitizers, called Analog Transient Waveform Digitizers, or ATWDs are used, one at a time, to digitize traces from events that trigger the DOMs threshold and are read out by their central data center. This pairing allows a significant reduction of dead time \cite{icebasic, icedom}.
%
%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=4.5 in]{./images/pevneuts.png}
%\caption[IceCube PeV Neutrino Events]{Pictured here are the two IceCube PeV neutrinos, where (a) is the earlier August 2011 event, and (b) is the January 2012 event. Plots from \cite{pevneuts}.}
%\label{pevneuts}
%\end{center}
%\end{figure}
%
%Neutrinos fluxes in general provide some of the best constraints on sources for UHECRs, since high energy hadronic interactions at the source of UHECRs should cause charged pions to emit relatively high energy neutrinos \cite{foteini,pulsarno}. It is frequently suggested that without a measurement of high energy astrophysical neutrinos, it is difficult to prove any acceleration mechanism. As of yet, IceCube has detected two PeV neutrinos (\autoref{pevneuts}) but nothing of higher energy. In general, models of cosmic accelerators desire 10$^17$ eV+ neutrinos.


\subsection{CTA}
\label{cta}
For the past 30 years, the Air Cherenkov has been the primary method for Very High Energy (VHE) gamma ray astronomy (with significant contributions from others such as HAWC and CASA-MIA). As many US and European groups began thinking about the next generation of Imaging Atmospheric Cherenkov Telescopes (IACTs),  it was realized that gamma ray physicists and astronomers from around the globe would have to pool resources to build a bigger array of telescopes. Hence, the Cherenkov Telescope Array (CTA) was conceived. In this subsection, we aim to introduce CTA and the pSCT MST to give context to \autoref{timatcta}.

Passing on the legacy of HESS, VERITAS and MAGIC, CTA will consist of two main sites, one in the Canary Islands, at La Palma on the Roque de Los Muchachos site, and another in Chile at the European Southern Observatory's Paranal site \cite{ctaong}. The sites have been chosen to maximize the performance per unit time, and minimize the cost of land and logistics \cite{tarek}. The array will consist of 3 or 4 telescope designs, with large and medium telescopes at the north site, and all three sizes of telescopes at the southern site. In air Cherenkov detectors, larger telescopes focus more light and thereby make lower energy events detectable, while multiple smaller telescopes allow for a wider field of view with less light collecting area, making them effective for high energy events.

Of the current designs for CTA telescopes, there is one model in the Large Sized Telescope (LST) category, two in the Medium Sized Telescope category (MST) and 3 in the Small Sized Telescope (SST). Ultimately, the collaboration will likely decide on 1 SST, while employing two MSTs of one type per site. The LST, one MST and one SST use the tried and proven Davies-Cotton optics design, while the rest use forms of two-mirror optics. In general, two mirror optics are more delicate, but allow for a wider field of view, and therefore more showers from gamma rays detected. A ``family photo" of the potential designs is shown in \autoref{ctades}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=5.9 in]{./images/ctades.png}
\caption[CTA Telescope Candidates]{In this figure, from \textcite{ctaong}, we can see all six of the candidate telescope designs\footnote{While a review of each design is outside the scope of this work, we would encourage those interested to see the paper this is from, or to simply search any of the above telescope names.}. }
\label{ctades}
\end{center}
\end{figure}

In CTA, the workhorse telescopes will be the Medium Sized Telescopes (MST). This is also the only design range in which deploying two different types of telescopes, with different optics and cameras, is being considered. To comment on this briefly, using two instruments can be more difficult than having a unified approach due to differing development, maintenance and analysis issues, but having them developed relatively independently can give important cross checks. Furthermore, from a political perspective, a country will understandably be more likely to fund the construction of an apparatus if scientists from the country played an important role in its development. From this perspective, accepting two MST designs can provide more infrastructure funding for both groups and sites.

To briefly take a broader view, in many of the Cosmic Accelerator candidates that I've discussed in \autoref{tabsec}, large leptonic populations are expected to be accelerated in addition to UHECRs. These leptons generally interact in the source and should produce VHE to UHE gamma-rays. With powerful enough optics, and a long enough exposure CTA and other gamma ray observatories should be able to find events at the top of their energy spectra which perhaps point back to UHECR sources. More data collection and higher statistics should aid in any future multi-messenger analysis involving VHE gamma-rays, UHE neutrinos and UHECRs. 

\subsubsection{Schwarzchild-Coud\`{e}r Telescope}
The northern MST is a design originally spawned by Vladimir Vassiliev of University of California, Los Angeles. This design was under consideration well before it became a candidate in CTA. Originally, this work was to culminate in the Advanced Gamma-ray Imaging System, but the team saw a more feasible funding opportunity by joining CTA \cite{agis}. With a mostly US and partly Italian team, the prototype of this telescope has been set up at the Fred Lawrence Whipple Observatory (which is, perhaps confusingly, not the site of the original Whipple Telescope). On Januray 31, 2019, this telescope saw first light. 

The optics and camera design make the Schwarzchild-Coud\`{e}r Telescope an advanced and unique design. More pixels will be active in the focal plane of this camera than every IACT currently in use (at time of writing) in the world, combined.

\begin{figure}[h!]
\begin{center}
\label{sctdiag}
\includegraphics[width=2.8 in]{./images/sctcam.png}
\includegraphics[width=2.8 in]{./images/sctoptics.png}
\caption[SCT Camera and Optics]{Left: The SCT camera design rendered, from \textcite{vladscope}. Note the detector modules being inserted into the lattice and the muffin fans mounted below for cooling. Right: A diagram of the camera's optics with a ray tracing simulation overlaid \cite{psctoptics}.}

\end{center}
\end{figure}

The two mirror Schwarzchild-Coud\`{e}r have been known about since the turn of the 20th century, but mirror finishing technology has only recently reached the needed precision-at-cost to make this telescope a viable option. The optics, shown in \autoref{sctdiag}, give this telescope an 8\degree field of view in each dimension, allowing a larger collecting area over the night sky than any previous or planned IACT \cite{psctoptics}. The optics also focus the image onto a focal plane that is narrower than previous designs, ruling out the use of PMTs in the camera.

The camera, due to optical constraints, must then use Silicon Photomultipliers (SiPMs), which are small silicon-based avalanche photodiodes, that effectively rely on the breakdown of their charge binding structure when put under a relatively high voltage (high for nature, low for PMTs). These allow us to fit 11.2 kPixels in the focal plane. At this resolution, it should be (and from the first couple of events, is) possible to accurate determine the morphology of the events, and therefore very efficiently reject cosmic ray events. Currently, the telescope is operating and needs a time-tagging system to allow for cross calibrations and meaningful event determination. This project is underway and will be discussed in \autoref{timatcta}.




